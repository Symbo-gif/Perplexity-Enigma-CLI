# Perplexity - Enigma Configuration
# Copy this file to .pplxrc and customize as needed

# ============================================================================
# API Configuration
# ============================================================================
api:
  key: ${PPLX_API_KEY}                    # Set via environment variable or here
  base_url: "https://api.perplexity.ai"
  timeout: 60000                           # Request timeout in milliseconds

# ============================================================================
# Model Selection
# ============================================================================
# Available models:
#   - sonar                   : Fast, lightweight (low cost)
#   - sonar-pro               : Balanced reasoning + search (medium cost)
#   - sonar-reasoning         : Chain-of-thought reasoning (medium cost)
#   - sonar-reasoning-pro     : Advanced reasoning (higher cost)
#   - sonar-reasoning-large   : Large context reasoning (higher cost)
#   - sonar-deep-research     : Comprehensive research (higher cost, 30-60s)
#   - sonar-large             : Large context model (higher cost)

models:
  default: "sonar-pro"                     # Default model for general queries
  search_heavy: "sonar-pro"                # Model for search-intensive queries
  reasoning: "sonar-reasoning-pro"         # Model for complex reasoning
  fast: "sonar"                            # Model for quick responses
  deep_research: "sonar-deep-research"     # Model for deep research tasks

# ============================================================================
# Agent Behavior
# ============================================================================
# These parameters control how the AI model behaves when generating responses
agent:
  max_iterations: 10                       # Max tool-use loops per query (not used yet)
  temperature: 0.3                         # 0.0-2.0, lower = more deterministic
  max_tokens: 4096                         # Max tokens in response
  top_p: 0.9                               # Nucleus sampling parameter (0.0-1.0)

# ============================================================================
# Search & Research Configuration
# ============================================================================
research:
  search_mode: "medium"                    # low | medium | high - controls search depth
  include_citations: true                  # Include source citations in responses
  focus_on_recent: true                    # Prefer recent sources when searching

# ============================================================================
# Output Configuration
# ============================================================================
output:
  format: "markdown"                       # markdown | json | plain
  stream: false                            # Stream responses in real-time (use --stream flag to override)
  verbose: false                           # Show debug information
